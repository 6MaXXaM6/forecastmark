{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d4b26bd3",
   "metadata": {},
   "source": [
    "## Задача - сделать модель что будет угадывать по напитку угадывать средний балл\n",
    "\n",
    "- достать данные\n",
    "- разметить их\n",
    "- обучить модель\n",
    "- дать доступ через телеграмм бот"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92ee401c",
   "metadata": {},
   "source": [
    "Исследование не претендует на научность - главная задача - развлечение"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d33f0e6",
   "metadata": {},
   "source": [
    "данные были собраны через Google Forms - я раскидал по чатам всех курсов ссылку на опрос результат сбора в табличке: - https://docs.google.com/spreadsheets/d/1GqODes6YSPXHsVeRFff1qCBORaGndstBX9WmngM2n1A/edit?gid=149531268#gid=149531268\n",
    "\n",
    "чтобы удобно экспортировать в csv и в процессе добавления ответов добавлять новые результаты быстро написал скрипт экспорта в csv для google apps script\n",
    "\n",
    "```\n",
    "function exportFilteredCSV() {\n",
    "  var startDate = new Date('2025-10-01'); // дата \n",
    "\n",
    "  var spreadsheet = SpreadsheetApp.getActiveSpreadsheet();\n",
    "  var sheet = spreadsheet.getSheetByName('Ответы');\n",
    "  var data = sheet.getDataRange().getValues();\n",
    "  \n",
    "  var dateColumnIndex = 0;\n",
    "  // Оставляем заголовки и фильтруем строки по дате\n",
    "  var headers = data[0];\n",
    "  var filteredData = data.filter(function(row, index) {\n",
    "    if (index === 0) return true;\n",
    "    var rowDate = new Date(row[dateColumnIndex]);\n",
    "    return rowDate > startDate;\n",
    "  });\n",
    "  \n",
    "  // Преобразуем отфильтрованные данные в CSV-текст\n",
    "  var csvContent = filteredData.map(function(row) {\n",
    "    return row.map(function(cell) {\n",
    "      cell = cell === null ? '' : cell.toString();\n",
    "      if (cell.includes(',') || cell.includes('\"') || cell.includes('\\n')) {\n",
    "        cell = '\"' + cell.replace(/\"/g, '\"\"') + '\"';\n",
    "      }\n",
    "      return cell;\n",
    "    }).join(',');\n",
    "  }).join('\\n');\n",
    "  \n",
    "  var blob = Utilities.newBlob(csvContent, 'text/csv', 'новые_данные.csv');\n",
    "  var file = DriveApp.createFile(blob);\n",
    "  Logger.log('Файл создан: ' + file.getUrl());\n",
    "}\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cedb69e8",
   "metadata": {},
   "source": [
    "Мы будем добавлять новые данные к старым данным - ручками оставляя релевантные данные"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3265dbff",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data has been successfully updated and saved to drinks.csv\n",
      "Total records: 88\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# === 1. Load the existing drinks.csv to get filter mappings ===\n",
    "try:\n",
    "    existing_drinks_df = pd.read_csv('drinks.csv')\n",
    "    \n",
    "    # Create case-insensitive mappings from existing data\n",
    "    favorite_mapping = {}\n",
    "    often_mapping = {}\n",
    "    \n",
    "    for _, row in existing_drinks_df.iterrows():\n",
    "        if pd.notna(row['like_drink']) and pd.notna(row['like_drink_f']):\n",
    "            key = str(row['like_drink']).lower().strip()\n",
    "            favorite_mapping[key] = row['like_drink_f']\n",
    "        \n",
    "        if pd.notna(row['often_drink']) and pd.notna(row['often_drink_f']):\n",
    "            key = str(row['often_drink']).lower().strip()\n",
    "            often_mapping[key] = row['often_drink_f']\n",
    "    \n",
    "except FileNotFoundError:\n",
    "    print(\"File drinks.csv not found. Starting from scratch.\")\n",
    "    existing_drinks_df = pd.DataFrame()\n",
    "    favorite_mapping = {}\n",
    "    often_mapping = {}\n",
    "\n",
    "# === 2. Load and process the new_data.csv ===\n",
    "new_data_df = pd.read_csv('new_data.csv')\n",
    "\n",
    "# Function to convert timestamp to UNIX format\n",
    "def convert_to_unix(timestamp_str):\n",
    "    date_str = timestamp_str[:33]\n",
    "    return pd.Timestamp(date_str).timestamp()\n",
    "\n",
    "new_data_df['timestamp_unix'] = new_data_df['Отметка времени'].apply(convert_to_unix)\n",
    "\n",
    "# Function to clean the 'money' column\n",
    "def clean_money(value):\n",
    "    if pd.isna(value) or value == '-':\n",
    "        return ''\n",
    "    value_str = str(value).lower().replace('k', '000').replace('к', '000')\n",
    "    value_str = ''.join(char for char in value_str if char.isdigit())\n",
    "    return value_str\n",
    "\n",
    "new_data_df['money'] = new_data_df['money'].apply(clean_money)\n",
    "\n",
    "# Drop the original timestamp column\n",
    "new_data_df.drop(columns=['Отметка времени'], inplace=True)\n",
    "\n",
    "# === 3. Create and populate the new filter columns using the mappings ===\n",
    "new_data_df['like_drink_f'] = None\n",
    "new_data_df['often_drink_f'] = None\n",
    "\n",
    "# Temporary storage for new mappings from current session\n",
    "new_favorite_mapping = favorite_mapping.copy()\n",
    "new_often_mapping = often_mapping.copy()\n",
    "\n",
    "for index, row in new_data_df.iterrows():\n",
    "    # For favorite drink\n",
    "    if pd.notna(row['like_drink']):\n",
    "        like_drink_key = str(row['like_drink']).lower().strip()\n",
    "        \n",
    "        if like_drink_key in new_favorite_mapping:\n",
    "            # Use existing mapping\n",
    "            new_data_df.at[index, 'like_drink_f'] = new_favorite_mapping[like_drink_key]\n",
    "        else:\n",
    "            # Ask for input and add to mapping\n",
    "            new_drink = input(f\"Введите любимый напиток для строки {index} (текущее: {row['like_drink']}): \")\n",
    "            new_data_df.at[index, 'like_drink_f'] = new_drink\n",
    "            new_favorite_mapping[like_drink_key] = new_drink\n",
    "    \n",
    "    # For frequent drink\n",
    "    if pd.notna(row['often_drink']):\n",
    "        often_drink_key = str(row['often_drink']).lower().strip()\n",
    "        \n",
    "        if often_drink_key in new_often_mapping:\n",
    "            # Use existing mapping\n",
    "            new_data_df.at[index, 'often_drink_f'] = new_often_mapping[often_drink_key]\n",
    "        else:\n",
    "            # Ask for input and add to mapping\n",
    "            new_drink = input(f\"Введите частый напиток для строки {index} (текущее: {row['often_drink']}): \")\n",
    "            new_data_df.at[index, 'often_drink_f'] = new_drink\n",
    "            new_often_mapping[often_drink_key] = new_drink\n",
    "\n",
    "# === 4. Combine with existing data and remove duplicates ===\n",
    "# Check if we have existing data to combine with\n",
    "if not existing_drinks_df.empty:\n",
    "    # Remove from new_data_df any rows that have timestamps already in existing_drinks_df\n",
    "    existing_timestamps = set(existing_drinks_df['timestamp_unix'])\n",
    "    new_data_df = new_data_df[~new_data_df['timestamp_unix'].isin(existing_timestamps)]\n",
    "    \n",
    "    # Concatenate only if there are new rows\n",
    "    if not new_data_df.empty:\n",
    "        combined_df = pd.concat([existing_drinks_df, new_data_df], ignore_index=True)\n",
    "    else:\n",
    "        combined_df = existing_drinks_df\n",
    "        print(\"No new data to add - all timestamps already exist.\")\n",
    "else:\n",
    "    combined_df = new_data_df\n",
    "\n",
    "# Reset the index for a clean order\n",
    "combined_df.reset_index(drop=True, inplace=True)\n",
    "\n",
    "# === 5. Save the updated combined dataset ===\n",
    "combined_df.to_csv('drinks.csv', index=False)\n",
    "\n",
    "print(f\"Data has been successfully updated and saved to drinks.csv\")\n",
    "print(f\"Total records: {len(combined_df)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da0ace95",
   "metadata": {},
   "source": [
    "обучим модель взял XGBOOST по идее catboost бы лучше отработал но данные мусорные см R2 - фактически мы по приколу обучили генератор рандомных чисел жаль( я ожидал от любителей пива 5.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6b982cfe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Размер данных до удаления пропусков: 88\n",
      "Размер данных после удаления пропусков: 86\n",
      "Типы данных в X:\n",
      "tier_encoded             int64\n",
      "frequency_day            int64\n",
      "frequency_day_o          int64\n",
      "like_drink_f_encoded     int64\n",
      "often_drink_f_encoded    int64\n",
      "dtype: object\n",
      "Тип данных в y: float64\n",
      "Размер обучающей выборки: (81, 5)\n",
      "Размер тестовой выборки: (5, 5)\n",
      "\n",
      "Начинаем K-Fold кросс-валидацию (5 фолдов)...\n",
      "\n",
      "Тестируем комбинацию параметров 1/4: {'n_estimators': 50, 'learning_rate': 0.1, 'max_depth': 4}\n",
      "Фолд 1: MSE = 0.6884, R2 = -0.3411\n",
      "Фолд 2: MSE = 0.4283, R2 = -0.7611\n",
      "Фолд 3: MSE = 0.4031, R2 = 0.0907\n",
      "Фолд 4: MSE = 0.7796, R2 = -0.4005\n",
      "Фолд 5: MSE = 0.6879, R2 = -0.7268\n",
      "Средние по фолдам: MSE = 0.5974, R2 = -0.4278\n",
      "НОВАЯ ЛУЧШАЯ МОДЕЛЬ! MSE = 0.5974\n",
      "\n",
      "Тестируем комбинацию параметров 2/4: {'n_estimators': 100, 'learning_rate': 0.1, 'max_depth': 6}\n",
      "Фолд 1: MSE = 0.6209, R2 = -0.2097\n",
      "Фолд 2: MSE = 0.5373, R2 = -1.2090\n",
      "Фолд 3: MSE = 0.4298, R2 = 0.0304\n",
      "Фолд 4: MSE = 0.7731, R2 = -0.3888\n",
      "Фолд 5: MSE = 0.6438, R2 = -0.6161\n",
      "Средние по фолдам: MSE = 0.6010, R2 = -0.4787\n",
      "\n",
      "Тестируем комбинацию параметров 3/4: {'n_estimators': 100, 'learning_rate': 0.05, 'max_depth': 4}\n",
      "Фолд 1: MSE = 0.6347, R2 = -0.2367\n",
      "Фолд 2: MSE = 0.4167, R2 = -0.7134\n",
      "Фолд 3: MSE = 0.4439, R2 = -0.0014\n",
      "Фолд 4: MSE = 0.7102, R2 = -0.2759\n",
      "Фолд 5: MSE = 0.6833, R2 = -0.7154\n",
      "Средние по фолдам: MSE = 0.5778, R2 = -0.3886\n",
      "НОВАЯ ЛУЧШАЯ МОДЕЛЬ! MSE = 0.5778\n",
      "\n",
      "Тестируем комбинацию параметров 4/4: {'n_estimators': 5000, 'learning_rate': 0.1, 'max_depth': 20}\n",
      "Фолд 1: MSE = 0.6991, R2 = -0.3621\n",
      "Фолд 2: MSE = 0.5472, R2 = -1.2500\n",
      "Фолд 3: MSE = 0.3793, R2 = 0.1443\n",
      "Фолд 4: MSE = 0.9867, R2 = -0.7727\n",
      "Фолд 5: MSE = 0.7170, R2 = -0.7998\n",
      "Средние по фолдам: MSE = 0.6659, R2 = -0.6081\n",
      "\n",
      "ФИНАЛЬНАЯ ОЦЕНКА ЛУЧШЕЙ МОДЕЛИ\n",
      "Лучшие параметры: {'n_estimators': 100, 'learning_rate': 0.05, 'max_depth': 4}\n",
      "Средний MSE на кросс-валидации: 0.5778\n",
      "\n",
      "Результаты на тестовых данных:\n",
      "Среднеквадратичная ошибка (MSE): 0.2794\n",
      "Коэффициент детерминации (R2): -0.5878\n",
      "Диапазон предсказаний: от 3.79 до 4.54\n",
      "\n",
      "Модель успешно сохранена!\n",
      "best_xgboost_model.json - основная модель\n",
      "best_xgboost_model.joblib - модель в joblib формате\n",
      "model_info.joblib - информация о модели\n",
      "label_encoders.joblib - кодировщики категориальных признаков\n",
      "Процесс завершен! Лучшая модель сохранена с MSE = 0.2794 на тестовых данных\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from xgboost import XGBRegressor\n",
    "from sklearn.model_selection import train_test_split, KFold\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "import joblib\n",
    "import re\n",
    "\n",
    "# Загрузка данных\n",
    "df = pd.read_csv('drinks.csv')\n",
    "\n",
    "# Функция для очистки балла\n",
    "def clean_numeric_value(value, default=4.0):\n",
    "    if pd.isna(value) or value == '':\n",
    "        return default\n",
    "    value_str = str(value).strip().replace(',', '.')\n",
    "    cleaned = re.sub(r'[^\\d\\.]', '', value_str)\n",
    "    \n",
    "    try:\n",
    "        result = float(cleaned)\n",
    "        # Ограничиваем значение в диапазоне от 2 до 5 от умников\n",
    "        return max(2.0, min(5.0, result))\n",
    "    except ValueError:\n",
    "        return default\n",
    "\n",
    "# Функция для очистки категориальных значений\n",
    "def clean_categorical_value(value):\n",
    "    if pd.isna(value):\n",
    "        return None\n",
    "    cleaned = str(value).strip().lower()\n",
    "    return cleaned if cleaned != '' else None\n",
    "\n",
    "# Очищаем категориальные столбцы\n",
    "categorical_columns = ['tier', 'like_drink_f', 'often_drink_f']\n",
    "for col in categorical_columns:\n",
    "    df[col] = df[col].apply(clean_categorical_value)\n",
    "\n",
    "# Очищаем числовые столбцы\n",
    "numeric_columns = ['mark']\n",
    "for col in numeric_columns:\n",
    "    df[col] = df[col].apply(clean_numeric_value)\n",
    "\n",
    "print(f\"Размер данных до удаления пропусков: {len(df)}\")\n",
    "# Удаляем строки, где пропущены категориальные признаки или целевая переменная\n",
    "df = df.dropna(subset=categorical_columns + numeric_columns)\n",
    "\n",
    "print(f\"Размер данных после удаления пропусков: {len(df)}\")\n",
    "\n",
    "# Векторизация категориальных признаков с помощью LabelEncoder\n",
    "label_encoders = {}\n",
    "for col in categorical_columns:\n",
    "    le = LabelEncoder()\n",
    "    df[col + '_encoded'] = le.fit_transform(df[col])\n",
    "    label_encoders[col] = le\n",
    "\n",
    "# Определение признаков и целевой переменной\n",
    "features = ['tier_encoded', 'frequency_day', 'frequency_day_o', 'like_drink_f_encoded', 'often_drink_f_encoded']\n",
    "X = df[features]\n",
    "y = df['mark']\n",
    "\n",
    "# Проверяем типы данных\n",
    "print(f\"Типы данных в X:\")\n",
    "print(X.dtypes)\n",
    "print(f\"Тип данных в y: {y.dtype}\")\n",
    "\n",
    "# Разделение на обучающую и тестовую выборки чтобы отследить как обучилась скорее по фолдам поймём чисто символически оставляю пару строк\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.05, random_state=52)\n",
    "\n",
    "print(f\"Размер обучающей выборки: {X_train.shape}\")\n",
    "print(f\"Размер тестовой выборки: {X_test.shape}\")\n",
    "    \n",
    "\n",
    "# K-FOLD КРОСС-ВАЛИДАЦИЯ ДЛЯ ПОИСКА ЛУЧШЕЙ МОДЕЛИ\n",
    "def clip_predictions(predictions):\n",
    "    return np.clip(predictions, 2.0, 5.0)\n",
    "\n",
    "# Настройки K-Fold\n",
    "n_splits = min(5, len(X_train) - 1)  # Адаптируем количество фолдов под объем данных\n",
    "kf = KFold(n_splits=n_splits, shuffle=True, random_state=42)\n",
    "\n",
    "# Параметры для перебора\n",
    "param_combinations = [\n",
    "    {'n_estimators': 50, 'learning_rate': 0.1, 'max_depth': 4},\n",
    "    {'n_estimators': 100, 'learning_rate': 0.1, 'max_depth': 6},\n",
    "    {'n_estimators': 100, 'learning_rate': 0.05, 'max_depth': 4},\n",
    "    {'n_estimators': 5000, 'learning_rate': 0.1, 'max_depth': 20}, # по приколу\n",
    "]\n",
    "\n",
    "best_model = None\n",
    "best_mse = np.inf\n",
    "best_params = None\n",
    "best_fold_results = []\n",
    "\n",
    "print(f\"\\nНачинаем K-Fold кросс-валидацию ({n_splits} фолдов)...\")\n",
    "\n",
    "for param_idx, params in enumerate(param_combinations):\n",
    "    print(f\"\\nТестируем комбинацию параметров {param_idx + 1}/{len(param_combinations)}: {params}\")\n",
    "\n",
    "    fold_scores = []\n",
    "    fold_models = []\n",
    "    \n",
    "    for fold, (train_idx, val_idx) in enumerate(kf.split(X_train)):\n",
    "        X_fold_train, X_fold_val = X_train.iloc[train_idx], X_train.iloc[val_idx]\n",
    "        y_fold_train, y_fold_val = y_train.iloc[train_idx], y_train.iloc[val_idx]\n",
    "        \n",
    "        model = XGBRegressor(**params, random_state=52)\n",
    "        model.fit(X_fold_train, y_fold_train)\n",
    "        \n",
    "        y_fold_pred = model.predict(X_fold_val)\n",
    "        y_fold_pred_clipped = clip_predictions(y_fold_pred)\n",
    "        \n",
    "        fold_r2 = r2_score(y_fold_val, y_fold_pred_clipped)\n",
    "        fold_mse = mean_squared_error(y_fold_val, y_fold_pred_clipped)\n",
    "        \n",
    "        fold_scores.append({'fold': fold, 'r2': fold_r2, 'mse': fold_mse})\n",
    "        fold_models.append(model)\n",
    "        \n",
    "        print(f\"Фолд {fold + 1}: MSE = {fold_mse:.4f}, R2 = {fold_r2:.4f}\")\n",
    "    \n",
    "    avg_mse = np.mean([score['mse'] for score in fold_scores])\n",
    "    avg_r2 = np.mean([score['r2'] for score in fold_scores])\n",
    "    \n",
    "    print(f\"Средние по фолдам: MSE = {avg_mse:.4f}, R2 = {avg_r2:.4f}\")\n",
    "    \n",
    "    if avg_mse < best_mse:\n",
    "        best_mse = avg_mse\n",
    "        best_params = params\n",
    "        best_fold_idx = np.argmin([score['mse'] for score in fold_scores])\n",
    "        best_model = fold_models[best_fold_idx]\n",
    "        best_fold_results = fold_scores\n",
    "        print(f\"НОВАЯ ЛУЧШАЯ МОДЕЛЬ! MSE = {avg_mse:.4f}\")\n",
    "\n",
    "print(\"\\nФИНАЛЬНАЯ ОЦЕНКА ЛУЧШЕЙ МОДЕЛИ\")\n",
    "print(f\"Лучшие параметры: {best_params}\")\n",
    "print(f\"Средний MSE на кросс-валидации: {best_mse:.4f}\")\n",
    "\n",
    "y_pred = best_model.predict(X_test)\n",
    "y_pred_clipped = clip_predictions(y_pred)\n",
    "\n",
    "final_mse = mean_squared_error(y_test, y_pred_clipped)\n",
    "final_r2 = r2_score(y_test, y_pred_clipped)\n",
    "\n",
    "print(f\"\\nРезультаты на тестовых данных:\")\n",
    "print(f\"Среднеквадратичная ошибка (MSE): {final_mse:.4f}\")\n",
    "print(f\"Коэффициент детерминации (R2): {final_r2:.4f}\")\n",
    "print(f\"Диапазон предсказаний: от {y_pred_clipped.min():.2f} до {y_pred_clipped.max():.2f}\")\n",
    "\n",
    "# Сохраняем модель и энкодеры\n",
    "best_model.save_model('best_xgboost_model.json')\n",
    "joblib.dump(best_model, 'best_xgboost_model.joblib')\n",
    "\n",
    "model_info = {\n",
    "    'best_params': best_params,\n",
    "    'test_r2': float(final_r2),\n",
    "    'test_mse': float(final_mse),\n",
    "    'features': features,\n",
    "}\n",
    "\n",
    "joblib.dump(model_info, 'model_info.joblib')\n",
    "joblib.dump(label_encoders, 'label_encoders.joblib')\n",
    "\n",
    "print(\"\\nМодель успешно сохранена!\")\n",
    "print(\"best_xgboost_model.json - основная модель\")\n",
    "print(\"best_xgboost_model.joblib - модель в joblib формате\")\n",
    "print(\"model_info.joblib - информация о модели\")\n",
    "print(\"label_encoders.joblib - кодировщики категориальных признаков\")\n",
    "\n",
    "print(f\"Процесс завершен! Лучшая модель сохранена с MSE = {final_mse:.4f} на тестовых данных\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31a6d9cd",
   "metadata": {},
   "source": [
    "ну раз сделали модель надо её запихнуть в прод в тг бот смеха ради"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1cf70743",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from xgboost import XGBRegressor\n",
    "import re\n",
    "import joblib\n",
    "\n",
    "def clean_numeric_value(value, default=4.0):\n",
    "    if pd.isna(value) or value == '':\n",
    "        return default\n",
    "    value_str = str(value).strip().replace(',', '.')\n",
    "    cleaned = re.sub(r'[^\\d\\.]', '', value_str)\n",
    "    try:\n",
    "        result = float(cleaned)\n",
    "        return max(2.0, min(5.0, result))\n",
    "    except ValueError:\n",
    "        return default\n",
    "\n",
    "def clean_categorical_value(value):\n",
    "    if pd.isna(value):\n",
    "        return None\n",
    "    return str(value).strip().lower()\n",
    "\n",
    "# Загрузка модели из JSON\n",
    "def load_xgboost_model(model_path='best_xgboost_model.json'):\n",
    "    model = XGBRegressor()\n",
    "    model.load_model(model_path)\n",
    "    print(f\"Модель загружена из {model_path}\")\n",
    "    return model\n",
    "\n",
    "# Загрузка LabelEncoders\n",
    "def load_label_encoders(encoders_path='label_encoders.joblib'):\n",
    "    label_encoders = joblib.load(encoders_path)\n",
    "    print(f\"LabelEncoders загружены из {encoders_path}\")\n",
    "    return label_encoders\n",
    "\n",
    "# Основная функция предсказания\n",
    "def predict_mark(model, label_encoders, tier, frequency_day, frequency_day_o, like_drink_f, often_drink_f):\n",
    "    # Очищаем входные данные\n",
    "    tier_clean = clean_categorical_value(tier)\n",
    "    frequency_day_clean = clean_numeric_value(frequency_day)\n",
    "    frequency_day_o_clean = clean_numeric_value(frequency_day_o)\n",
    "    like_drink_f_clean = clean_categorical_value(like_drink_f)\n",
    "    often_drink_f_clean = clean_categorical_value(often_drink_f)\n",
    "    \n",
    "    # Создаем DataFrame для новых данных\n",
    "    new_data = pd.DataFrame({\n",
    "        'tier': [tier_clean],\n",
    "        'frequency_day': [frequency_day_clean],\n",
    "        'frequency_day_o': [frequency_day_o_clean],\n",
    "        'like_drink_f': [like_drink_f_clean],\n",
    "        'often_drink_f': [often_drink_f_clean]\n",
    "    })\n",
    "    \n",
    "    categorical_columns = ['tier', 'like_drink_f', 'often_drink_f']\n",
    "    \n",
    "    for col in categorical_columns:\n",
    "        le = label_encoders[col]\n",
    "        try:\n",
    "            new_data[col + '_encoded'] = le.transform(new_data[col])\n",
    "        except ValueError:\n",
    "            # Если значение новое, используем наиболее частый класс (0)\n",
    "            new_data[col + '_encoded'] = 0\n",
    "            print(f\"Новое значение '{new_data[col].iloc[0]}' для признака '{col}'\")\n",
    "    \n",
    "    # Используем закодированные признаки\n",
    "    features = ['tier_encoded', 'frequency_day', 'frequency_day_o', 'like_drink_f_encoded', 'often_drink_f_encoded']\n",
    "    features_new = new_data[features]\n",
    "    \n",
    "    # Предсказываем и ограничиваем результат\n",
    "    prediction = model.predict(features_new)[0]\n",
    "    clipped_prediction = max(2.0, min(5.0, prediction))\n",
    "    \n",
    "    return round(clipped_prediction, 2)\n",
    "\n",
    "class MarkPredictor:\n",
    "    def __init__(self, model_path='best_xgboost_model.json', encoders_path='label_encoders.joblib'):\n",
    "        self.model = load_xgboost_model(model_path)\n",
    "        self.label_encoders = load_label_encoders(encoders_path)\n",
    "    \n",
    "    def predict(self, tier, frequency_day, frequency_day_o, like_drink_f, often_drink_f):\n",
    "        return predict_mark(self.model, self.label_encoders, tier, frequency_day, frequency_day_o, like_drink_f, often_drink_f)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cs_dl",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
